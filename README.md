# Minimal BPE Tokenizer

## ðŸ“Œ Overview
This is a minimal implementation of a **Byte Pair Encoding (BPE) tokenizer** in Python, created for the GDG ML recruitment task.

The tokenizer can:
1. Train on a text corpus (e.g., `input.txt`)
2. Encode text into integer IDs
3. Decode IDs back to text (lossless for training data)

## ðŸ“‚ File Structure
bpe_tokenizer/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ tokenizer.py # Tokenizer implementation
â”‚ â””â”€â”€ main.py # Example runner
â”œâ”€â”€ input.txt # Training corpus (e.g., alice.txt)
â”œâ”€â”€ output.txt # Demo results
â””â”€â”€ README.md

markdown
Copy code

## âš¡ Usage
1. Place your training text in `input.txt`.
2. Run the example:
```bash
python src/main.py 
```
3. The encoded IDs and decoded text will be saved in output.txt